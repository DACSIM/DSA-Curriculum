{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9513b82c",
   "metadata": {},
   "source": [
    "# Introduction to GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7ba4a",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "1. Setting up the required variables to call the endpoint GrabGPT API\n",
    "2. Making chat compltion calls to library\n",
    "3. Handling intermittent network and rate limit issues gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80164b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv langchain-openai langchain-core langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fee8994-ca19-4fa0-b586-c8b94f4af0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure environment variables are loaded before accessing them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbce11",
   "metadata": {},
   "source": [
    "### Set the API Key environment\n",
    "- gpt-4\n",
    "- gpt-3.5-turbo\n",
    "- langsmith\n",
    "- Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db16bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf6cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ce8a9",
   "metadata": {},
   "source": [
    "### Introduction to LangSmith\n",
    "\n",
    "LangSmith is a tool in the Langchain ecosystem designed to help monitor, evaluate, and debug language models more effectively. It provides the infrastructure for logging interactions, running tests, and tracking metrics to ensure optimal model performance.\n",
    "\n",
    "We will be covering this topic in depth in future sessions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e187e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step: LangSmith\n",
    "\n",
    "\n",
    "# Let's you trace everything that is going on in this codespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0847e38",
   "metadata": {},
   "source": [
    "# What is GPT?\n",
    "\n",
    "GPT refers to a suite of powerful language models developed by OpenAI, such as **GPT-3**, **GPT-4**, and **GPT-4 with Vision**. These models can perform a wide variety of tasks, including text generation, conversation, translation, and image processing (in the case of GPT-4 with Vision).\n",
    "\n",
    "Through OpenAI's API, users can access these models directly via OpenAI’s platform, bypassing the need for third-party integrations like Azure. With an **OpenAI API key**, you have direct access to the latest models OpenAI offers.\n",
    "\n",
    "While our focus for this training is on OpenAI’s models, which are the most widely used and well-documented, the principles for interacting with other large language models (LLMs) like Meta’s LLaMA or Anthropic’s Claude are largely similar.\n",
    "\n",
    "## Available Models through OpenAI API:\n",
    "\n",
    "| Model Name                      | Description                               |\n",
    "|----------------------------------|-------------------------------------------|\n",
    "| **gpt-3.5-turbo**                | A highly efficient variant of GPT-3.5, great for most conversational tasks. |\n",
    "| **gpt-4**                        | The latest iteration of OpenAI’s powerful GPT series, offering enhanced understanding and reasoning abilities. |\n",
    "| **gpt-4-32k**                    | A larger version of GPT-4 with the ability to process longer context windows. |\n",
    "| **gpt-4 with Vision**            | Allows GPT-4 to process both text and images, useful for tasks that combine visual and textual inputs. |\n",
    "| **text-embedding-ada-002**       | A model specialized for creating embeddings for tasks like text similarity, search, and clustering. |\n",
    "| **Whisper**                      | OpenAI’s speech recognition model for transcribing and translating audio. |\n",
    "| **DALL·E**                       | OpenAI’s image generation model, capable of generating images from textual descriptions. |\n",
    "\n",
    "For a comprehensive list of models and capabilities available through OpenAI’s API, you can refer to the [OpenAI API documentation](https://platform.openai.com/docs).\n",
    "\n",
    "---\n",
    "\n",
    "This version reflects the use of the OpenAI API key for direct access to OpenAI's models, rather than relying on Azure or other providers. Let me know if you'd like further adjustments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt 4\n",
    "# Run once can le!\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc383b",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "The token limit determines how much information can be handled in a single interaction. For large documents or complex tasks, a higher token limit allows for more extensive input and output, while lower limits mean shorter interactions.\n",
    "\n",
    "Why is this important?\n",
    "The token limit determines how much information can be handled in a single interaction. For large documents or complex tasks, a higher token limit allows for more extensive input and output, while lower limits mean shorter interactions.\n",
    "\n",
    "GPT-4 (8k context model): This version has a maximum token limit of 8,192 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f78dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca539ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt 3.5 turbo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643b88d",
   "metadata": {},
   "source": [
    "# Try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11873c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Ask chatgpt for Singapore's most popular breakfast meal!\n",
    "2) Take note of the tokens used\n",
    "3) Find out the cost!\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab72657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b232f",
   "metadata": {},
   "source": [
    "## Prompt Engineering & Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b44198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate # Mimic what you will see when using ChatGPT UI\n",
    "\n",
    "# PART 1: Create a ChatPromptTemplate using a template string\n",
    "print(\"-----Prompt from Template-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d364f17",
   "metadata": {},
   "source": [
    "#### You will notice there is alot of room for dynamic changes using a prompt template as compared to using the standard chat template above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: Prompt with Multiple Placeholders\n",
    "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
    "template_multiple = \"\"\"You are a helpful assistant.\n",
    "Human: Tell me a {adjective} short story about a {animal}.\n",
    "Assistant:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out yourself\n",
    "\"\"\"\n",
    "Write a Python code snippet using ChatPromptTemplate to:\n",
    "\n",
    "1) Create the system and human messages using tuples.\n",
    "2) The system message should say: \"You are a calculus expert tutor.\"\n",
    "3) The human message should say: \"Help me solve {problem_count} calculus problems.\"\n",
    "4) Use problem_count = \"\\int_0^2 (3x^2 - 2x + 1) \\, dx\" as an input.\n",
    "5) Invoke the prompt and print the response from the model.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133dfbd",
   "metadata": {},
   "source": [
    "### Additional\n",
    "\n",
    "From lazy prompt to detailed prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05917b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can take prompts that were pre made by people!\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hardkothari/prompt-maker\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(new_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f625f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90f635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0d55772",
   "metadata": {},
   "source": [
    "## Few-Shot Prompt Template Example\n",
    "In this section, we'll learn how to create a simple prompt template that helps guide the model by providing example inputs and outputs. This technique, known as few-shotting, involves showing the model a few examples to help it understand the task better. It is a powerful way to improve the quality of the generated output, especially when the task is complex or context-dependent.\n",
    "\n",
    "A few-shot prompt template can be built from a fixed set of examples or can be dynamically constructed using an Example Selector class, which is responsible for selecting relevant examples from a pre-defined set based on the query.\n",
    "\n",
    "## Parameter Explanations\n",
    "\n",
    "#### Prompt Template: A framework that structures your prompt and integrates a set of few-shot examples to guide the model's behavior.\n",
    "\n",
    "#### Few-Shotting: Providing a series of example inputs and outputs to the model in the prompt. This helps the model generate better responses by mimicking the patterns in the examples.\n",
    "\n",
    "If you want to read more about [Few-Shot-Prompting Papers](https://arxiv.org/abs/2005.14165)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a49079",
   "metadata": {},
   "source": [
    "## Difference Between Zero-Shot, One-Shot, and Few-Shot Learning\n",
    "\n",
    "- **Zero-Shot Learning**: In zero-shot learning, the model is able to perform a task without having seen any examples or prior data for that specific task. It relies on knowledge transfer from other tasks or context provided by the model.\n",
    "\n",
    "- **One-Shot Learning**: In one-shot learning, the model is trained on only one example of each task or class and is expected to generalize well enough to perform accurately on new, unseen data.\n",
    "\n",
    "- **Few-Shot Learning**: In few-shot learning, the model is trained on a small number of examples (usually a handful) for each class or task, and it uses this limited data to make predictions or perform the task on new examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc582d",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbce165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot example: Sentiment analysis (classification task with no prior examples)\n",
    "\n",
    "# System message defines the assistant's role\n",
    "content=\"You are a helpful assistant. Who is great at picking up the nuances in a sentence and direct in the way you respond\"\n",
    "\n",
    "# Human message asks the model to classify the sentiment of the sentence\n",
    "content=\"Classify the following sentence as positive, negative, or neutral: 'The product quality is excellent and I love it!'\"\n",
    "\n",
    "# Send the conversation to the model\n",
    "\n",
    "\n",
    "# Output the model's response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c81a9d",
   "metadata": {},
   "source": [
    "## This can be problematic when the task is very ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"You are a helpful assistant. Who is great at picking up the nuances in a sentence and direct in the way you respond.\"\n",
    "\n",
    "content= \"Classify the following sentence as positive, negative, or neutral:\\\n",
    "    'The product works well most of the time, but there are moments when it suddenly stops working,\\\n",
    "    which can be frustrating. However, I think it's a decent option overall, and the design is nice,\\\n",
    "    though I expected a bit more durability for the price.'\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374eeda3",
   "metadata": {},
   "source": [
    "# One shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message defines the assistant's role\n",
    "\n",
    "\n",
    "# Human message provides a one-shot example of English-to-French translation and asks for a new translation\n",
    "\n",
    "\"Translate the following sentence from English to French:\\n\"\n",
    "\"Example: 'I love data science.' => 'J'adore la science des données.'\\n\"\n",
    "\"Now translate: 'Data is the new oil.'\"\n",
    "\n",
    "\n",
    "# Send the conversation to the model\n",
    "\n",
    "\n",
    "# Output the model's response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f8e17-7b59-48ec-8543-bae5a15a3899",
   "metadata": {},
   "source": [
    "## Few Shots\n",
    "\n",
    "One of the most effective ways to improve model performance is to give a model examples of what you want it to do. The technique of adding example inputs and expected outputs to a model prompt is known as \"few-shot prompting\". The technique is based on the Language Models are Few-Shot Learners paper. There are a few things to think about when doing few-shot prompting:\n",
    "\n",
    "How are examples generated?\n",
    "How many examples are in each prompt?\n",
    "How are examples selected at runtime?\n",
    "How are examples formatted in the prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa21146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import(\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7e466e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create some examples to show our AI what we want\n",
    "examples = [\n",
    "    {\"input\":\"2+2\", \"output\": \"4\"},\n",
    "    {\"input\":\"2+3\", \"output\": \"5\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an example_prompt\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b556f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b35de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in few_shot_prompt:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fb71dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"You are a wonderous wizard of math.\"\n",
    "human_template=\"{input}\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18877bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use chain here each item is called a runnable, \n",
    "# we will dive deeper in the future session\n",
    "\n",
    "\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06e580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Are follow up questions needed here: Yes.\n",
    "            Follow up: How old was Muhammad Ali when he died?\n",
    "            Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "            Follow up: How old was Alan Turing when he died?\n",
    "            Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "            So the final answer is: Muhammad Ali\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the founder of craigslist born?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Are follow up questions needed here: Yes.\n",
    "            Follow up: Who was the founder of craigslist?\n",
    "            Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "            Follow up: When was Craig Newmark born?\n",
    "            Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "            So the final answer is: December 6, 1952\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Are follow up questions needed here: Yes.\n",
    "            Follow up: Who was the mother of George Washington?\n",
    "            Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "            Follow up: Who was the father of Mary Ball Washington?\n",
    "            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "            So the final answer is: Joseph Ball                                     \n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Are follow up questions needed here: Yes.\n",
    "            Follow up: Who is the director of Jaws?\n",
    "            Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "            Follow up: Where is Steven Spielberg from?\n",
    "            Intermediate Answer: The United States.\n",
    "            Follow up: Who is the director of Casino Royale?\n",
    "            Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "            Follow up: Where is Martin Campbell from?\n",
    "            Intermediate Answer: New Zealand.\n",
    "            So the final answer is: No\n",
    "        \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1b43404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define the structure for individual examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f99cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the example prompt with the first example\n",
    "print(example_prompt.invoke(examples[0]))\n",
    "print(example_prompt.invoke(examples[0]).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "\n",
    ")\n",
    "\n",
    "print(\n",
    "    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "john_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
